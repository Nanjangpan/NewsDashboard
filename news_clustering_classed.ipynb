{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "news_clustering_classed",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leedhn/NewsDashboard/blob/nlp/news_clustering_classed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt4eS-LjrLdE",
        "outputId": "559c9576-ac1c-466b-d789-f2296f49e1aa"
      },
      "source": [
        "##DAHYUN\r\n",
        "\r\n",
        "!git clone https://github.com/SKTBrain/KoBERT.git\r\n",
        "%cd KoBERT\r\n",
        "!pip install -r requirements.txt\r\n",
        "!pip install .\r\n",
        "%cd ..\r\n",
        "!git clone https://github.com/BM-K/KoSentenceBERT_SKTBERT.git\r\n",
        "%cd KoSentenceBERT_SKTBERT\r\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoBERT'...\n",
            "remote: Enumerating objects: 155, done.\u001b[K\n",
            "remote: Total 155 (delta 0), reused 0 (delta 0), pack-reused 155\n",
            "Receiving objects: 100% (155/155), 179.81 KiB | 1.06 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "/content/KoBERT\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.7.0+cu101)\n",
            "Collecting mxnet>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 104kB/s \n",
            "\u001b[?25hCollecting gluonnlp>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 54.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.4MB/s \n",
            "\u001b[?25hCollecting onnxruntime>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/f009251fd1b91a2e1ce6f22d4b5be9936fbd0072842c5087a2a49706c509/onnxruntime-1.6.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 50.1MB/s \n",
            "\u001b[?25hCollecting transformers>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet>=1.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (20.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime>=0.3.0->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.6.0->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (51.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (1.0.0)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588527 sha256=69ca39377147d66a56c7f28a22f891edfb080628b1fba84f5b58c8f5cfc76567\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=4992498551287952ce74d515df6c566b480d053e25e3bc81779a79076fb2eac9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp sacremoses\n",
            "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, onnxruntime, sacremoses, tokenizers, transformers\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 mxnet-1.7.0.post1 onnxruntime-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.4 transformers-4.1.1\n",
            "Processing /content/KoBERT\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-cp36-none-any.whl size=12734 sha256=8df6f702b630989026b60f6c0777016aa1a4c1f13b9f5e101a3406ab7f43e600\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wwvlzhb4/wheels/d0/ca/3e/bd91a227ac542246f4e8f94ff2f7f12a1a1f0c5b77601bcccf\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n",
            "/content\n",
            "Cloning into 'KoSentenceBERT_SKTBERT'...\n",
            "remote: Enumerating objects: 505, done.\u001b[K\n",
            "remote: Counting objects: 100% (505/505), done.\u001b[K\n",
            "remote: Compressing objects: 100% (403/403), done.\u001b[K\n",
            "remote: Total 505 (delta 95), reused 493 (delta 94), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (505/505), 46.58 MiB | 17.77 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "Checking out files: 100% (448/448), done.\n",
            "/content/KoSentenceBERT_SKTBERT\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (4.41.1)\n",
            "Collecting sentence_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/9a/62beeb5501b70ab48b9e5bb92de290f00a661a1caa075c4aae56d452aaa0/sentence-transformers-0.4.0.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.7.0+cu101)\n",
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.1.94)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 7)) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 7)) (3.7.4.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r requirements.txt (line 8)) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r requirements.txt (line 8)) (2.23.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r requirements.txt (line 8)) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r requirements.txt (line 8)) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/c6/912cc2cfd1b4051621552fc5961c25e2f517a090d179a38f62d5cdaf5d37/boto3-1.16.43-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 59.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0->-r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->-r requirements.txt (line 8)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->-r requirements.txt (line 8)) (3.0.4)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.43\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/6c/9f6e6a14f53b21b6f1670ccd789082015911458823914b7dabca333ae033/botocore-1.19.43-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 44.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.43->boto3->transformers==2.8.0->-r requirements.txt (line 8)) (2.8.1)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.0-cp36-none-any.whl size=102655 sha256=d9e7aa622f9e7b7be9979886bb6045af6a0dbc108a6d22f631ca49ff92043807\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/76/65/50258d8b7930e909ea2f5bd006a23d520a16765af13ab45bb3\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: botocore 1.19.43 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sentence-transformers 0.4.0 has requirement transformers<5.0.0,>=3.1.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, transformers, sentence-transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed boto3-1.16.43 botocore-1.19.43 jmespath-0.10.0 s3transfer-0.3.3 sentence-transformers-0.4.0 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyQPXHi_qT7E",
        "outputId": "79a49c04-3590-49d9-a97b-0c63fe155748"
      },
      "source": [
        "%cd ..\r\n",
        "# !pip install boto3\r\n",
        "# !pip install s3fs\r\n",
        "# !pip uninstall transformers --yes\r\n",
        "# !pip install transformers\r\n",
        "# !pip uninstall tokenizers --yes\r\n",
        "# !pip install tokenizers\r\n",
        "!pip install -r requirements.txt\r\n",
        "%cd KoSentenceBERT_SKTBERT"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Requirement already satisfied: boto3==1.16.43 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.16.43)\n",
            "Collecting s3fs==0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/58/732ea1c735d725b1cc4cf365ae6326c22569a5e88c8502d13844e91f08ef/s3fs-0.5.1-py3-none-any.whl\n",
            "Collecting tokenizers==0.9.4\n",
            "  Using cached https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Collecting transformers==4.1.1\n",
            "  Using cached https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3==1.16.43->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.43 in /usr/local/lib/python3.6/dist-packages (from boto3==1.16.43->-r requirements.txt (line 1)) (1.19.43)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3==1.16.43->-r requirements.txt (line 1)) (0.10.0)\n",
            "Collecting aiobotocore>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/25/a81b015035012131056a6b7a339eec052f86f33e35fd91f160e961ea2a5e/aiobotocore-1.1.2-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1->-r requirements.txt (line 4)) (1.19.4)\n",
            "Collecting urllib3<1.27,>=1.25.4; python_version != \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.43->boto3==1.16.43->-r requirements.txt (line 1)) (2.8.1)\n",
            "Collecting aioitertools>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/32/0b/3260ac050de07bf6e91871944583bb8598091da19155c34f7ef02244709c/aioitertools-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.6/dist-packages (from aiobotocore>=1.0.1->s3fs==0.5.1->-r requirements.txt (line 2)) (1.12.1)\n",
            "Collecting aiohttp>=3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 27.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.1->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1->-r requirements.txt (line 4)) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: typing_extensions>=3.7 in /usr/local/lib/python3.6/dist-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs==0.5.1->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 55.4MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 60.6MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==0.5.1->-r requirements.txt (line 2)) (20.3.0)\n",
            "Building wheels for collected packages: idna-ssl\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=3280c2c7291cdf44a254c623040c4a2aa5eb459d5898c346b7cc4c897de2cf5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built idna-ssl\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: aiobotocore 1.1.2 has requirement botocore<1.17.45,>=1.17.44, but you'll have botocore 1.19.43 which is incompatible.\u001b[0m\n",
            "Installing collected packages: aioitertools, multidict, yarl, async-timeout, idna-ssl, aiohttp, aiobotocore, fsspec, s3fs, tokenizers, transformers, urllib3\n",
            "  Found existing installation: tokenizers 0.5.2\n",
            "    Uninstalling tokenizers-0.5.2:\n",
            "      Successfully uninstalled tokenizers-0.5.2\n",
            "  Found existing installation: transformers 2.8.0\n",
            "    Uninstalling transformers-2.8.0:\n",
            "      Successfully uninstalled transformers-2.8.0\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed aiobotocore-1.1.2 aiohttp-3.7.3 aioitertools-0.7.1 async-timeout-3.0.1 fsspec-0.8.5 idna-ssl-1.1.0 multidict-5.1.0 s3fs-0.5.1 tokenizers-0.9.4 transformers-4.1.1 urllib3-1.26.2 yarl-1.6.3\n",
            "/content/KoSentenceBERT_SKTBERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo1wnMrvwAlZ",
        "outputId": "9197337a-fad3-4de3-9452-36d4ce33ac50"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPsxU83WO3p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b431c6-131a-4edf-86c1-13f644a30c84"
      },
      "source": [
        "##DAHYUN\n",
        "#만약 SentenceTransformer경로 에러가 뜬다면...일단 import된 패키지를 지워야 하니까! 아래 주석을 돌리세요.\n",
        "#del SentenceTransformer,util\n",
        "#os.path.abspath(inspect.getfile(SentenceTransformer))\n",
        "%ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clustering.py        \u001b[0m\u001b[01;34moutput\u001b[0m/           SemanticSearch.py       training_nli.py\n",
            "con_training_sts.py  README.md         \u001b[01;34msentence_transformers\u001b[0m/  training_sts.py\n",
            "\u001b[01;34mKorNLUDatasets\u001b[0m/      requirements.txt  \u001b[01;34mtokenizers\u001b[0m/             \u001b[01;34mtransformers\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZw7jSJmF_eW",
        "outputId": "fcfae81e-6dba-40c3-a2c8-3ee751b674bf"
      },
      "source": [
        "#%cd /KoSentenceBERT_SKTBERT/\n",
        "#%ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoSentenceBERT_SKTBERT\n",
            "Clustering.py        \u001b[0m\u001b[01;34moutput\u001b[0m/           SemanticSearch.py       training_nli.py\n",
            "con_training_sts.py  README.md         \u001b[01;34msentence_transformers\u001b[0m/  training_sts.py\n",
            "\u001b[01;34mKorNLUDatasets\u001b[0m/      requirements.txt  \u001b[01;34mtokenizers\u001b[0m/             \u001b[01;34mtransformers\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIFHfZNMy7bM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb846f04-13ac-4438-e360-3e9f91c21dbd"
      },
      "source": [
        "import boto3\n",
        "import json\n",
        "import pandas as pd\n",
        "import s3fs\n",
        "import os\n",
        "import inspect\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import seaborn as sns; sns.set_theme()\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import psycopg2 as pg2\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "np.random.seed(0)\n",
        "\n",
        "pd.set_option('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None)\n",
        "\n",
        "eps={'오피니언':1,'세계':0.65,'사회':1.1 ,'경제':1.15 ,'생활문화':0.75 ,'IT과학':1 }\n",
        "min_samples={'오피니언':3,'세계':3,'사회':5,'경제':5 ,'생활문화':3 ,'IT과학' :3 }\n",
        "\n",
        "class Clusteringnews:\n",
        "    def __init__(self):\n",
        "        self.section=''\n",
        "        self.df_merge= pd.DataFrame(columns=[\"category\",\n",
        "                                             \"aid\",\n",
        "                                             \"date\",\n",
        "                                             \"title\",\n",
        "                                             \"content\",\n",
        "                                             \"company\"])\n",
        "        self.model_path = 'output/training_sts'\n",
        "       \n",
        "\n",
        "    def bring_input_data_from_s3(self,arg0,arg1): # key에 들어가는 cate와 날짜\n",
        "        self.section=arg0\n",
        "        s3 = boto3.client('s3',\n",
        "        #key 삭제해야함 ㅠㅠ \n",
        "            aws_access_key_id='',\n",
        "            aws_secret_access_key='')\n",
        "        key_list=s3.list_objects_v2(Bucket='naver-news-dev')\n",
        "\n",
        "        df_key = pd.DataFrame(columns=[\"name\",\n",
        "            \"section\",\n",
        "            \"date\",\n",
        "            \"start_date\",\n",
        "            \"end_date\",\n",
        "            \"num\",                               \n",
        "            \"key\"])\n",
        "        key_temp = 0\n",
        "        for i in key_list['Contents']: \n",
        "            #print(i['Key'])\n",
        "            try:\n",
        "                name,section,date=i['Key'].split('/')\n",
        "                start_date,end_date,num,trash = date.split('_')\n",
        "                df_key.loc[key_temp] = [name,section,date,start_date,end_date,num,i['Key']]\n",
        "                key_temp = key_temp +1\n",
        "            except:\n",
        "                print('error')\n",
        "        df_key=df_key[(df_key['section']==arg0) &  (df_key['start_date']==arg1)]['key']\n",
        "        print(df_key)\n",
        "        \n",
        "        for i in df_key:\n",
        "            obj = s3.get_object(Bucket='naver-news-dev', Key=i)\n",
        "            data = json.loads(obj['Body'].read())\n",
        "            df = pd.DataFrame(data)\n",
        "            df=df.transpose()\n",
        "            self.df_merge=pd.concat([self.df_merge,df],axis=0)\n",
        "        self.df_merge['aid'] = self.df_merge['aid'].astype(str)\n",
        "        \n",
        "\n",
        "    def bring_input_data_from_post(self,arg0): # category\n",
        "        conn=pg2.connect(host=\"ybigta-news.cga4jkz7xhwb.ap-northeast-2.rds.amazonaws.com\",user=\"news\",password=\"newsdashboard!\", database = 'dev',port=\"5432\")\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        query = \"\"\"SELECT  case when category = 100 then '정치'\n",
        "                    when category = 101 then '경제'\n",
        "                    when category = 102 then '사회'\n",
        "                    when category = 103 then '생활문화'\n",
        "                    when category = 105 then 'IT과학'\n",
        "                    when category = 110 then '오피니언' end as category\n",
        "                    , id as aid, date(ni.datetime) as date, title, content, press as company\n",
        "                    FROM news_info ni\n",
        "                    where ni.datetime > now() - (36000 * interval '1 sec') \n",
        "                    and category = '\"\"\" + arg0 + \"\"\"'\n",
        "                    \"\"\"\n",
        "        cur.execute(query)\n",
        "        rows = cur.fetchall()\n",
        "        conn.commit()\n",
        "        self.df_merge = pd.DataFrame(rows,columns=[\"category\",\n",
        "        \"aid\",\n",
        "        \"date\",\n",
        "        \"title\",\n",
        "        \"content\",\n",
        "        \"company\"])\n",
        "        self.df_merge['aid'] = self.df_merge['aid'].astype(str)\n",
        "        return self.df_merge\n",
        "\n",
        "\n",
        "    def bring_input_data_from_mongo(arg0):\n",
        "        temp_df = pd.DataFrame(arg0)\n",
        "        temp_df=temp_df.transpose()\n",
        "        return temp_df\n",
        "\n",
        "    #SentenceBERT로 뉴스 제목들 임베딩해서 리턴\n",
        "    def embedding(self,corpus_raw,embedder):\n",
        "\n",
        "        \n",
        "        corpus=corpus_raw[0:16]\n",
        "        corpus_embeddings = embedder.encode(corpus)\n",
        "        corpus=self.df_title['title'].tolist()[16:32]\n",
        "\n",
        "        corpus_raw_16=len(corpus_raw)//16\n",
        "        for i in range(corpus_raw_16-1):\n",
        "            corpus=corpus_raw[16*(i+1):16*(i+2)]\n",
        "            corpus_embeddings=np.concatenate((corpus_embeddings,embedder.encode(corpus)),axis=0)\n",
        "        if (len(corpus_raw)>corpus_raw_16*16):\n",
        "            corpus=corpus_raw[corpus_raw_16*16:] \n",
        "        corpus_embeddings=np.concatenate((corpus_embeddings,embedder.encode(corpus)),axis=0)\n",
        "        \n",
        "        return corpus_embeddings\n",
        "\n",
        "    #클러스터 내 코사인유사도 구해서 리스트로 반환\n",
        "    def cs_score(self,gc,gc_id,gc_raw):\n",
        "        cs_score=0\n",
        "        self.cs_scores=[0 for i in range(len(gc))]\n",
        "\n",
        "        for i in range(len(gc)):\n",
        "            cs_score=0\n",
        "            cs_score=sum(sum(cosine_similarity(gc[i],gc[i])))-len(gc[i])\n",
        "            if len(gc[i])==1:\n",
        "                self.cs_scores[i]=cs_score\n",
        "            else:\n",
        "                self.cs_scores[i] =cs_score/((len(gc[i])*(len(gc[i])-1)))#/2)\n",
        "        return self.cs_scores\n",
        "\n",
        "    #클러스터-리스트 리턴\n",
        "    #프린트문 나중에 지울 것.\n",
        "    def cluster_list(self,gc_id,gc_raw, cs_scores):\n",
        "\n",
        "        final_cluster_raw=[]\n",
        "        final_cluster_news_id=[]\n",
        "        final_cluster_cs_score=[]\n",
        "\n",
        "        for i in range(len(gc_id)):\n",
        "            if cs_scores[i]<0.95:\n",
        "                continue\n",
        "\n",
        "            final_cluster_news_id.append(gc_id[i]) #최종으로 살아남은 클러스터 별 뉴스 id\n",
        "            final_cluster_raw.append(gc_raw[i]) #최종으로 살아남은 클러스터별 뉴스 제목\n",
        "            final_cluster_cs_score.append(cs_scores[i]) #최종으로 살아남은 클러스터 별 코사인 유사도\n",
        "        \n",
        "        return final_cluster_raw,final_cluster_news_id,final_cluster_cs_score    \n",
        "    \n",
        "    #DAHYUN\n",
        "    #클러스터 정렬 후 딕셔너리로 리턴.\n",
        "    def make_cluster_dic(self,final_cluster_raw,final_cluster_news_id,final_cluster_cs_score):\n",
        "        print('here')\n",
        "        cluster_dic_raw={}\n",
        "        cluster_dic={}\n",
        "        for i in range(len(final_cluster_raw)):\n",
        "            cluster_dic_raw[i]={}\n",
        "            cluster_dic_raw[i]['id']=final_cluster_news_id[i]\n",
        "            cluster_dic_raw[i]['cs_score']=final_cluster_cs_score[i]\n",
        "            cluster_dic_raw[i]['size']=len(final_cluster_news_id[i])    \n",
        "        #클러스터 딕셔너리를 뉴스기사 개수 순으로 정렬해요\n",
        "        cluster_dic_raw=sorted(cluster_dic_raw.items(),key=(lambda x: x[1]['size']),reverse=True)\n",
        "        \n",
        "        for i in range(len(final_cluster_news_id)):\n",
        "            cluster_dic[i]={}\n",
        "            cluster_dic[i]['id']=cluster_dic_raw[i][1]['id']\n",
        "            cluster_dic[i]['cs_score']=cluster_dic_raw[i][1]['cs_score']\n",
        "            cluster_dic[i]['size']=cluster_dic_raw[i][1]['size']\n",
        "        print(len(final_cluster_news_id))\n",
        "        return cluster_dic\n",
        "\n",
        "    #실제로 클러스터링 하는 메인 함수\n",
        "    def clustering_function(self):\n",
        "        \n",
        "        #SentenceTransformer 모델 정의 = embedder \n",
        "        embedder = SentenceTransformer(self.model_path)\n",
        "\n",
        "        #df_title 정의\n",
        "        df_t=self.df_merge.copy()\n",
        "        self.df_title=df_t[['category','aid','date','title']]\n",
        "\n",
        "        #corpus_raw: 뉴스 제목들 리스트\n",
        "        corpus_raw=self.df_title['title'].tolist()\n",
        "\n",
        "        #뉴스 제목들 embedder 사용해서 벡터표현으로 바꿈.\n",
        "        print('embedding start')\n",
        "        corpus_embeddings=self.embedding(corpus_raw, embedder)\n",
        "        print('embedding finish')\n",
        "        #PCA\n",
        "        pca2 = PCA(n_components = 2)                #for plt\n",
        "        X2D = pca2.fit_transform(corpus_embeddings) #for plt\n",
        "        pca3 = PCA(n_components = 10)\n",
        "        X3D = pca3.fit_transform(corpus_embeddings)\n",
        "\n",
        "        #표준화(평균=0, 분산=1)\n",
        "        scale = StandardScaler()                    #for plt\n",
        "        scale.fit(X2D)                              #for plt\n",
        "        scaled_X = scale.transform(X2D)             #for plt\n",
        "        scaled_x = scaled_X[:,0]                    #for plt\n",
        "        scaled_y = scaled_X[:,1]                    #for plt\n",
        "\n",
        "        #DBSCAN\n",
        "        #parameters-----------------------------------------------\n",
        "        eps={'오피니언':1,'세계':0.65,'사회':1.1 ,'경제':1.15 ,'생활문화':0.75 ,'IT과학':1 }\n",
        "        min_samples={'오피니언':3,'세계':5,'사회':5,'경제':5 ,'생활문화':3 ,'IT과학' :3 }\n",
        "\n",
        "        #print('dbscan start')\n",
        "        dbscan = DBSCAN(eps=eps[self.section], min_samples=min_samples[self.section]) #사회.\n",
        "\n",
        "        cluster = dbscan.fit_predict(X3D)\n",
        "        #print('dbscan finish')\n",
        "\n",
        "        #클러스터링 된 거 plt로 점 찍어서 보여줌.-----------------\n",
        "        #plt.scatter(x=scaled_x,y=scaled_y,c=cluster)\n",
        "        \n",
        "        #print(max(cluster)) #클러스터 개수\n",
        "        #print(len(cluster)) #should be matched with news number\n",
        "\n",
        "        #클러스터링 하는 데에 사용----------------------------\n",
        "        group_cluster=[[] for i in range(len(cluster))] \n",
        "        group_raw=[[]for i in range(len(cluster))] \n",
        "        group_news_id=[[]for i in range(len(cluster))]\n",
        "        #-----------------------------------------------\n",
        "\n",
        "        gc=[] #벡터표현 들어감 (클러스터링 된 리스트)\n",
        "        gc_raw=[] #뉴스 제목 한글로 들어감 (클러스터링 된 리스트)\n",
        "        gc_id=[] #뉴스 아이디 (클러스터링 된 리스트)\n",
        "\n",
        "        np_ids=self.df_title['aid'].tolist()\n",
        "        #-----------------------------------------------\n",
        "        for i in range(len(corpus_raw)):\n",
        "            group_cluster[cluster[i]].append(X3D[i])\n",
        "            group_raw[cluster[i]].append(corpus_raw[i])\n",
        "            group_news_id[cluster[i]].append(np_ids[i])\n",
        "\n",
        "        num=0 #클러스터링 된 뉴스 개수들. \n",
        "        for i in range(len(cluster)):\n",
        "            if i==len(cluster)-1 or group_cluster[i]==[]:\n",
        "                continue\n",
        "            gc.append(group_cluster[i])\n",
        "            gc_raw.append(group_raw[i])\n",
        "            gc_id.append(group_news_id[i])\n",
        "            num+=len(group_raw[i])\n",
        "\n",
        "        #DBSCAN으로 클러스터링\n",
        "        #각 클러스터별로 코사인유사도 구해서 0.95이상만 살림\n",
        "        #클러스터별 코사인 유사도는 cs_scores리스트에 저장됨. 클러스터 인덱스와 같은 인덱스를 사용한다.\n",
        "        #클러스터별로 묶인 벡터 리스트는 gc\n",
        "        #클러스터 별로 묶인 벡터의 원본 스트링(기사제목)은 gc_raw에 저장되어있어요.\n",
        "        #print('cs score calculation start')\n",
        "        cs_scores=self.cs_score(gc,gc_id,gc_raw)\n",
        "        #print('cs score calculation finish')\n",
        "        \n",
        "        final_cluster_raw,final_cluster_news_id,final_cluster_cs_score=self.cluster_list(gc_id,gc_raw,cs_scores)\n",
        "\n",
        "        self.cluster_dic=self.make_cluster_dic(final_cluster_raw,\n",
        "                                          final_cluster_news_id,\n",
        "                                          final_cluster_cs_score)\n",
        "        #print('clustering finish')\n",
        "        return (self.cluster_dic)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqcBS7HXdepe"
      },
      "source": [
        "def get_section_dic(sections,date):\r\n",
        "    cn=Clusteringnews()\r\n",
        "    section_dic={}\r\n",
        "\r\n",
        "    for section in sections:\r\n",
        "        cluster_dic={}\r\n",
        "        cn.bring_input_data_from_s3(section,date)\r\n",
        "        cluster_dic=cn.clustering_function()\r\n",
        "\r\n",
        "        section_dic[section]=cluster_dic\r\n",
        "    return section_dic"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vo-MOJheKni",
        "outputId": "0fa352c8-bd4d-4d51-bae5-6d70d903b11e"
      },
      "source": [
        "sections=['IT과학','사회','세계','오피니언','경제','생활문화']\r\n",
        "section_dic=get_section_dic(sections,'20200922')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error\n",
            "error\n",
            "error\n",
            "10    news_content/IT과학/20200922_20200923_0_9361.json\n",
            "11    news_content/IT과학/20200922_20200923_1_9361.json\n",
            "12    news_content/IT과학/20200922_20200923_2_9361.json\n",
            "Name: key, dtype: object\n",
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}